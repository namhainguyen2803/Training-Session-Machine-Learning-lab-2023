{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/nguyennamhai/opt/anaconda3/envs/workplace/lib/python3.10/site-packages (1.23.5)\n",
      "Requirement already satisfied: pandas in /Users/nguyennamhai/opt/anaconda3/envs/workplace/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/nguyennamhai/opt/anaconda3/envs/workplace/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/nguyennamhai/opt/anaconda3/envs/workplace/lib/python3.10/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/nguyennamhai/opt/anaconda3/envs/workplace/lib/python3.10/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nguyennamhai/opt/anaconda3/envs/workplace/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for os\u001b[0m\n",
      "Requirement already satisfied: nltk in /Users/nguyennamhai/opt/anaconda3/envs/workplace/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/nguyennamhai/opt/anaconda3/envs/workplace/lib/python3.10/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in /Users/nguyennamhai/opt/anaconda3/envs/workplace/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in /Users/nguyennamhai/opt/anaconda3/envs/workplace/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/nguyennamhai/opt/anaconda3/envs/workplace/lib/python3.10/site-packages (from nltk) (2022.10.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install os\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2327,
     "status": "ok",
     "timestamp": 1673022162022,
     "user": {
      "displayName": "Nam Hải Nguyễn",
      "userId": "03702515726667605785"
     },
     "user_tz": -420
    },
    "id": "CwofXvKNIsvT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from nltk.stem import PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1673022162022,
     "user": {
      "displayName": "Nam Hải Nguyễn",
      "userId": "03702515726667605785"
     },
     "user_tz": -420
    },
    "id": "GOkx5FyXI7D7"
   },
   "outputs": [],
   "source": [
    "link_drive = \"/Users/nguyennamhai/HUST/Training phase Machine Learning lab 2023/Phase 1/Session 1/dataset/20news-bydate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1466,
     "status": "ok",
     "timestamp": 1673022163485,
     "user": {
      "displayName": "Nam Hải Nguyễn",
      "userId": "03702515726667605785"
     },
     "user_tz": -420
    },
    "id": "anUg7QJ5JyOc",
    "outputId": "8a40a270-16e2-4e66-8574-8b486202187f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'stop_words.txt', '20news-bydate-test', '20news-bydate-train']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(link_drive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1673022163486,
     "user": {
      "displayName": "Nam Hải Nguyễn",
      "userId": "03702515726667605785"
     },
     "user_tz": -420
    },
    "id": "wWfr7SQBK73w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nguyennamhai/HUST/Training phase Machine Learning lab 2023/Phase 1/Session 1/dataset/20news-bydate/20news-bydate-train\n",
      "/Users/nguyennamhai/HUST/Training phase Machine Learning lab 2023/Phase 1/Session 1/dataset/20news-bydate/20news-bydate-test\n",
      "/Users/nguyennamhai/HUST/Training phase Machine Learning lab 2023/Phase 1/Session 1/dataset/20news-bydate/stop_words.txt\n"
     ]
    }
   ],
   "source": [
    "train_dir = [link_drive + \"/\" + path for path in os.listdir(link_drive) if \"train\" in path][0]\n",
    "test_dir = [link_drive + \"/\" + path for path in os.listdir(link_drive) if \"test\" in path][0]\n",
    "stop_words_dir = [link_drive + \"/\" + path for path in os.listdir(link_drive) if \"stop_words\" in path][0]\n",
    "print(train_dir)\n",
    "print(test_dir)\n",
    "print(stop_words_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1673022163486,
     "user": {
      "displayName": "Nam Hải Nguyễn",
      "userId": "03702515726667605785"
     },
     "user_tz": -420
    },
    "id": "M1N6TbK-WiGj"
   },
   "outputs": [],
   "source": [
    "# Remove duplicate words in stop_words.txt\n",
    "def remove_duplicate_words_in_stop_words_files():\n",
    "  with open(stop_words_dir, \"r\") as f:\n",
    "    stop_words = f.read().splitlines()\n",
    "  print(len(stop_words))\n",
    "  stopwords = list()\n",
    "  for word in stop_words:\n",
    "    if word not in stopwords:\n",
    "      stopwords.append(word)\n",
    "  print(len(stopwords))\n",
    "  with open(stop_words_dir, \"w\") as f:\n",
    "    for word in stopwords:\n",
    "      f.write(word)\n",
    "      f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1673022163486,
     "user": {
      "displayName": "Nam Hải Nguyễn",
      "userId": "03702515726667605785"
     },
     "user_tz": -420
    },
    "id": "zVxIAoSDeaMY",
    "outputId": "268d276c-3320-423b-8d59-963330f17b77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "487\n"
     ]
    }
   ],
   "source": [
    "with open(stop_words_dir, \"r\") as f:\n",
    "  stop_words = f.read().splitlines()\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1673022163486,
     "user": {
      "displayName": "Nam Hải Nguyễn",
      "userId": "03702515726667605785"
     },
     "user_tz": -420
    },
    "id": "mUxHJv5vLoqo",
    "outputId": "e5f7f270-742a-4000-a993-ddafe4c5194c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "list_newsgroups = os.listdir(train_dir)\n",
    "list_newsgroups.sort()\n",
    "list_newsgroups = list_newsgroups[2:]\n",
    "print(list_newsgroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "R2QcBpbRfJng"
   },
   "outputs": [],
   "source": [
    "# Gather all data from all groups\n",
    "stemmer = PorterStemmer()\n",
    "def collect_data_from(parent_dir, newsgroup_list):\n",
    "  data = []\n",
    "  seen_filename = list()\n",
    "  files = list()\n",
    "  for group_id, newsgroup in enumerate(newsgroup_list):\n",
    "    label = group_id\n",
    "    dir_path = parent_dir + '/' + newsgroup + '/'\n",
    "    for filename in os.listdir(dir_path):\n",
    "      if os.path.isfile(dir_path + filename) and filename not in seen_filename:\n",
    "        seen_filename.append(filename)\n",
    "        files.append([filename, dir_path + filename])\n",
    "    files.sort()\n",
    "\n",
    "    for filename, filepath in files:\n",
    "      with open(filepath, errors='ignore') as f:\n",
    "        text = f.read().lower()\n",
    "        # Remove stop words and stemming words\n",
    "        words = [stemmer.stem(word) for word in re.split('\\W+', text) if word not in stop_words]\n",
    "        content = ' '.join(words)\n",
    "        assert len(content.splitlines()) == 1\n",
    "        data.append(str(label) + '<fff>' + filename + '<fff>' + content)\n",
    "  return data  \n",
    "\n",
    "data_train = collect_data_from(train_dir, list_newsgroups)\n",
    "data_test = collect_data_from(test_dir, list_newsgroups)\n",
    "data_full = data_train + data_test\n",
    "\n",
    "# Write data to file with w+\n",
    "with open(link_drive + \"/20news_train_processed.txt\", \"w+\") as f:\n",
    "    f.write(\"\\n\".join(data_train))\n",
    "with open(link_drive + \"/20news_test_processed.txt\", \"w+\") as f:\n",
    "    f.write(\"\\n\".join(data_test))\n",
    "with open(link_drive + \"/20news_full_processed.txt\", \"w+\") as f:\n",
    "    f.write(\"\\n\".join(data_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1466,
     "status": "ok",
     "timestamp": 1673019752655,
     "user": {
      "displayName": "Nam Hải Nguyễn",
      "userId": "03702515726667605785"
     },
     "user_tz": -420
    },
    "id": "OpnTRFSgqA11",
    "outputId": "76eeb2f2-6f2d-4f5d-8048-ce2658554c8d"
   },
   "outputs": [],
   "source": [
    "# Generate Dictionary(Bag of Words)\n",
    "with open(link_drive + \"/20news_full_processed.txt\", errors='ignore') as f:\n",
    "    lines = f.read().splitlines()\n",
    "doc_count = dict()\n",
    "corpus_size = len(lines)\n",
    "\n",
    "for line in lines:\n",
    "    features = line.split('<fff>')\n",
    "    text = features[-1]\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        doc_count[word] = doc_count.get(word, 0) + 1\n",
    "print(len(doc_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1673020274320,
     "user": {
      "displayName": "Nam Hải Nguyễn",
      "userId": "03702515726667605785"
     },
     "user_tz": -420
    },
    "id": "z0dglrDuv4x6"
   },
   "outputs": [],
   "source": [
    "# Compute IDF:\n",
    "def compute_idf(doc_freq, corpus_size):\n",
    "  assert doc_freq > 0\n",
    "  return np.log10(corpus_size * 1 / doc_freq)\n",
    "\n",
    "words_idfs = list()\n",
    "for word, document_freq in doc_count.items():\n",
    "  if document_freq > 10 and word.isdigit() == False:\n",
    "    words_idfs.append([word, compute_idf(document_freq, corpus_size)])\n",
    "\n",
    "words_idfs.sort(key=lambda word_idf: -word_idf[1])\n",
    "# Write dictionary file \n",
    "with open(link_drive + \"/words_idfs.txt\", 'w+') as f:\n",
    "    f.write('\\n'.join([word + '<fff>' + str(idf) for word, idf in words_idfs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOTjSF_yxf4X"
   },
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "print(0)\n",
    "with open(link_drive + \"/words_idfs.txt\", errors='ignore') as f:\n",
    "  lines = f.read().splitlines()\n",
    "\n",
    "words_idfs = list()\n",
    "words_IDs = dict()\n",
    "\n",
    "for line in lines:\n",
    "  l = line.split('<fff>')\n",
    "  words_idfs.append([l[0], float(l[1])])\n",
    "for index, (word, idf) in enumerate(words_idfs):\n",
    "  words_IDs[word] = index\n",
    "idfs = dict(words_idfs)\n",
    "print(1)\n",
    "with open(link_drive + \"/20news_full_processed.txt\", errors='ignore') as f:\n",
    "  lies = f.read().splitlines()\n",
    "print(2)\n",
    "documents = list()\n",
    "for lie in lies:\n",
    "  l = lie.split('<fff>')\n",
    "  documents.append([l[0], float(l[1]), l[2]])\n",
    "  data_tf_idf = []\n",
    "  for document in documents:\n",
    "    label, doc_id, text = document\n",
    "    words = [word for word in text.split() if word in idfs]\n",
    "    word_set = list(set(words))\n",
    "    max_term_freq = max([words.count(word) for word in word_set])\n",
    "    words_tfidfs = []\n",
    "    sum_squares = 0.0\n",
    "\n",
    "    for word in word_set:\n",
    "      term_freq = words.count(word)\n",
    "      tf_idf_value = term_freq * (1. / max_term_freq) * idfs[word]\n",
    "      words_tfidfs.append([words_IDs[word], tf_idf_value])\n",
    "      sum_squares += tf_idf_value ** 2\n",
    "\n",
    "  words_tfidfs_normalize = [str(index) + ':'+ str(tf_idf_value / np.sqrt(sum_squares)) for index, tf_idf_value in words_tfidfs]\n",
    "  sparse_rep = ' '.join(words_tfidfs_normalize)\n",
    "  data_tf_idf.append([label, doc_id, sparse_rep])\n",
    "print(3)\n",
    "# Write file data_tf_idf with abc\n",
    "with open(link_drive + \"/data_tf_idf.txt\", 'w+') as f:\n",
    "  result = []\n",
    "  for num in range(len(data_tf_idf)):\n",
    "    connect = \"\".join([str(data_tf_idf[num][0]), str(data_tf_idf[num][1]), data_tf_idf[num][2]])\n",
    "    result.append(connect)\n",
    "  f.write('\\n'.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMc+yR5NrJrQEU9Yxjx/8oY",
   "mount_file_id": "1cHydWlaPqlTpAUIdYhC7xIhDWiE8E8Vt",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
