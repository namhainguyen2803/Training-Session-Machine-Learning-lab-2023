{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VVeYXJvcnHtH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data paths\n",
        "\n",
        "news_bydata_path = '//Users/nguyennamhai/HUST/Training phase Machine Learning lab 2023/Phase 1/Session 1/Datasets/20news-bydate'"
      ],
      "metadata": {
        "id": "kU7xJ6mCsCaF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing data\n",
        "def gen_data_and_vocab():\n",
        "    def collect_data_from(parent_path, newsgroup_list, word_count=None):\n",
        "        data = []\n",
        "        for group_id, newsgroup in enumerate(newsgroup_list):\n",
        "            dir_path = parent_path + '/' + newsgroup + '/'\n",
        "\n",
        "            files = [(filename, dir_path + filename)\n",
        "                     for filename in os.listdir(dir_path)\n",
        "                     if os.path.isfile(dir_path + filename)]\n",
        "            files.sort()\n",
        "            label = group_id\n",
        "            print('Processing: {}-{}'.format(group_id, newsgroup))\n",
        "\n",
        "            for filename, filepath in files:\n",
        "                with open(filepath) as f:\n",
        "                    text = f.read().lower()\n",
        "                    words = re.split('\\W+', text)\n",
        "                    if word_count is not None:\n",
        "                        for word in words:\n",
        "                            word_count[word] += 1\n",
        "                    content = ' '.join(words)\n",
        "                    assert len(content.splitlines()) == 1\n",
        "                    data.append(str(label) + '<fff>' + filename + '<fff>' + content)\n",
        "        return data\n",
        "\n",
        "    word_count = defaultdict(int)\n",
        "\n",
        "    path = news_bydata_path\n",
        "    parts = [path + '/' + dir_name + '/' for dir_name in os.listdir(path)\n",
        "             if not os.path.isfile(path + dir_name)]\n",
        "\n",
        "    train_path, test_path = (parts[0], parts[1]) if 'train' in parts[0] else (parts[1], parts[0])\n",
        "\n",
        "    newsgroup_list = [newsgroup for newsgroup in os.listdir(train_path)]\n",
        "    newsgroup_list.sort()\n",
        "\n",
        "    train_data = collect_data_from(\n",
        "        parent_path=train_path,\n",
        "        newsgroup_list=newsgroup_list,\n",
        "        word_count=word_count\n",
        "    )\n",
        "\n",
        "    vocab = [word for word, freq in zip(word_count.keys(), word_count.values()) if freq > 10]\n",
        "    vocab.sort()\n",
        "    with open('/Users/nguyennamhai/HUST/Training phase Machine Learning lab 2023/Phase 1/Session 4/Datasets/vocab_raw.txt', 'w') as f:\n",
        "        f.write('\\n'.join(vocab))\n",
        "\n",
        "    test_data = collect_data_from(parent_path=test_path, newsgroup_list=newsgroup_list)\n",
        "\n",
        "    with open('/Users/nguyennamhai/HUST/Training phase Machine Learning lab 2023/Phase 1/Session 4/Datasets/20news_train_raw.txt', 'w') as f:\n",
        "        f.write('\\n'.join(train_data))\n",
        "\n",
        "    with open('/Users/nguyennamhai/HUST/Training phase Machine Learning lab 2023/Phase 1/Session 4/Datasets/20news_test_raw.txt', 'w') as f:\n",
        "        f.write('\\n'.join(test_data))"
      ],
      "metadata": {
        "id": "WvnT8c0UoTBf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_data_and_vocab()"
      ],
      "metadata": {
        "id": "32P1SMsltaye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding data\n",
        "\n",
        "unknown_ID = 0\n",
        "padding_ID = 1\n",
        "MAX_SENTENCE_LENGTH = 500\n",
        "\n",
        "def encode_data(data_path, vocab_path):\n",
        "    with open(vocab_path) as f:\n",
        "        vocab = dict([(word, word_ID + 2)\n",
        "                      for word_ID, word in enumerate(f.read().splitlines())])\n",
        "    with open(data_path) as f:\n",
        "        documents = [(line.split('<fff>')[0], line.split('<fff>')[1], line.split('<fff>')[2])\n",
        "                     for line in f.read().splitlines()]\n",
        "\n",
        "    encoded_data = []\n",
        "    for document in documents:\n",
        "        label, doc_id, text = document\n",
        "        words = text.split()[:MAX_SENTENCE_LENGTH]\n",
        "        sentence_length = len(words)\n",
        "        encoded_text = []\n",
        "        for word in words:\n",
        "            if word in vocab:\n",
        "                encoded_text.append(str(vocab[word]))\n",
        "            else:\n",
        "                encoded_text.append(str(unknown_ID))\n",
        "\n",
        "        if len(words) < MAX_SENTENCE_LENGTH:\n",
        "            num_padding = MAX_SENTENCE_LENGTH - len(words)\n",
        "            for _ in range(num_padding):\n",
        "                encoded_text.append(str(padding_ID))\n",
        "\n",
        "        encoded_data.append(str(label) + '<fff>' + str(doc_id) + '<fff>'\n",
        "                            + str(sentence_length) + '<fff>' + ' '.join(encoded_text))\n",
        "\n",
        "    dir_name = '/'.join(data_path.split('/')[:-1])\n",
        "    file_name = '-'.join(data_path.split('/')[-1].split('-')[:-1]) + '-encoded.txt'\n",
        "    with open(dir_name + '/' + file_name, 'w') as f:\n",
        "        f.write('\\n'.join(encoded_data))"
      ],
      "metadata": {
        "id": "siC5B5itp17h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_path = '/Users/nguyennamhai/HUST/Training phase Machine Learning lab 2023/Phase 1/Session 4/Datasets/20news_train_raw.txt'\n",
        "test_data_path = '/Users/nguyennamhai/HUST/Training phase Machine Learning lab 2023/Phase 1/Session 4/Datasets/20news_test_raw.txt'\n",
        "vocab_path = '/Users/nguyennamhai/HUST/Training phase Machine Learning lab 2023/Phase 1/Session 4/Datasets/vocab_raw.txt'\n",
        "\n",
        "encode_data(train_data_path, vocab_path)\n",
        "encode_data(test_data_path, vocab_path)"
      ],
      "metadata": {
        "id": "3lvbx8PgqWaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM model\n",
        "\n",
        "MAX_SENTENCE_LENGTH = 500\n",
        "NUM_CLASSES = 20\n",
        "\n",
        "class RNN:\n",
        "  def __init__(self, vocab_size, embedding_size, lstm_size, batch_size):\n",
        "    self._vocab_size = vocab_size\n",
        "    self._embedding_size = embedding_size\n",
        "    self._lstm_size = lstm_size\n",
        "    self._batch_size = batch_size\n",
        "\n",
        "    self._data = tf.placeholder(tf.int32, shape=[batch_size, MAX_SENTENCE_LENGTH])\n",
        "    self._labels = tf.placeholder(tf.int32, shape=[batch_size, ])\n",
        "    self._sentence_lengths = tf.placeholder(tf.int32, shape=[batch_size, ])\n",
        "    self._final_tokens = tf.placeholder(tf.int32, shape=[batch_size, ])\n",
        "\n",
        "\n",
        "  def embedding_layer(self, indices):\n",
        "    pretrained_vectors = []\n",
        "    pretrained_vectors.append(np.zeros(self._embedding_size))\n",
        "    np.random.seed(2021)\n",
        "    for _ in range (self._vocab_size + 1):\n",
        "      pretrained_vectors.append(np.random.normal(loc=0., scale=1., size=self._embedding_size))\n",
        "\n",
        "    pretrained_vectors = np.array(pretrained_vectors)\n",
        "\n",
        "    self._embedding_matrix = tf.get_variable(\n",
        "        name='embedding',\n",
        "        shape=(self._vocab_size + 2, self._embedding_size),\n",
        "        initializer=tf.constant_initializer(pretrained_vectors)\n",
        "    )\n",
        "    return tf.nn.embedding_lookup(self._embedding_matrix, indices)\n",
        "\n",
        "\n",
        "  def LSTM_layer(self, embeddings):\n",
        "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(self._lstm_size)\n",
        "    zero_state = tf.zeros(shape=(self._batch_size, self._lstm_size))\n",
        "    initial_state = tf.nn.rnn_cell.LSTMStateTuple(zero_state, zero_state)\n",
        "\n",
        "    lstm_inputs = tf.unstack(\n",
        "        tf.transpose(embeddings, perm=[1, 0, 2])\n",
        "    )\n",
        "    lstm_outputs, last_state = tf.nn.static_rnn(\n",
        "        cell=lstm_cell,\n",
        "        inputs=lstm_inputs,\n",
        "        initial_state=initial_state,\n",
        "        sequence_length=self._sentence_lengths\n",
        "    )\n",
        "    lstm_outputs = tf.unstack(\n",
        "        tf.transpose(lstm_outputs, perm=[1, 0, 2])\n",
        "    )\n",
        "    lstm_outputs = tf.concat(\n",
        "        lstm_outputs,\n",
        "        axis=0\n",
        "    )\n",
        "    mask = tf.sequence_mask(\n",
        "        lengths=self._sentence_lengths,\n",
        "        maxlen=MAX_SENTENCE_LENGTH,\n",
        "        dtype=tf.float32\n",
        "    ) \n",
        "    mask = tf.concat(tf.unstack(mask, axis=0), axis=0)\n",
        "    mask = tf.expand_dims(mask, -1)\n",
        "\n",
        "    lstm_outputs = mask * lstm_outputs\n",
        "    lstm_outputs_split = tf.split(lstm_outputs, num_or_size_splits=self._batch_size)\n",
        "    lstm_outputs_sum = tf.reduce_sum(lstm_outputs_split, axis=1)\n",
        "    lstm_outputs_average = lstm_outputs_sum / tf.expand_dims(\n",
        "        tf.cast(self._sentence_lengths, tf.float32),\n",
        "        -1\n",
        "    ) \n",
        "\n",
        "    return lstm_outputs_average\n",
        "\n",
        "  def build_graph(self):\n",
        "    embeddings = self.embedding_layer(self._data)\n",
        "    lstm_outputs = self.LSTM_layer(embeddings)\n",
        "\n",
        "    weigths = tf.get_variable(\n",
        "        name = 'final_layer_weights',\n",
        "        shape = (self._lstm_size, NUM_CLASSES),\n",
        "        initializer = tf.random_normal_initializer(seed = 2021)\n",
        "    )\n",
        "    biases = tf.get_variable(\n",
        "        name = 'final_layer_biases',\n",
        "        shape = (NUM_CLASSES),\n",
        "        initializer = tf.random_normal_initializer(seed = 2021)\n",
        "    )\n",
        "    logits = tf.matmul(lstm_outputs, weigths) + biases\n",
        "\n",
        "    labels_one_hot = tf.one_hot(\n",
        "        indices = self._labels,\n",
        "        depth = NUM_CLASSES,\n",
        "        dtype = tf.float32\n",
        "    )\n",
        "\n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits(\n",
        "        labels = labels_one_hot,\n",
        "        logits = logits\n",
        "    )\n",
        "    loss = tf.reduce_mean(loss)\n",
        "\n",
        "    probs = tf.nn.softmax(logits)\n",
        "    predicted_labels = tf.argmax(probs, axis = 1)\n",
        "    predicted_labels = tf.squeeze(predicted_labels)\n",
        "    return predicted_labels, loss\n",
        "\n",
        "\n",
        "  def trainer(self, loss, learning_rate):\n",
        "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "    return train_op"
      ],
      "metadata": {
        "id": "A3w5i9Ssql9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataReader\n",
        "class DataReader:\n",
        "  def __init__(self, data_path, batch_size):\n",
        "    self._batch_size = batch_size\n",
        "    with open(data_path) as f:\n",
        "      d_lines = f.read().splitlines()\n",
        "\n",
        "    self._data = []\n",
        "    self._labels = []\n",
        "    self._sentence_lengths = []\n",
        "    self._final_tokens = [] \n",
        "    for data_id, line in enumerate(d_lines):\n",
        "      features = line.split('')\n",
        "      label, doc_id, sentence_length = int(features[0]), int(features[1]), int(features[2])\n",
        "      tokens = features[3].split()\n",
        "\n",
        "      self._data.append(tokens)\n",
        "      self._sentence_lengths.append(sentence_length)\n",
        "      self._labels.append(label)\n",
        "      self._final_tokens.append(tokens[-1])\n",
        "\n",
        "    self._data = np.array(self._data)\n",
        "    self._labels = np.array(self._labels)\n",
        "    self._sentence_lengths = np.array(self._sentence_lengths)\n",
        "    self._final_tokens = np.array(self._final_tokens)\n",
        "\n",
        "    self._num_epoch = 0\n",
        "    self._batch_id = 0\n",
        "    self._size = len(self._data)\n",
        "\n",
        "  def next_batch(self):\n",
        "    start = self._batch_id * self._batch_size\n",
        "    end = start + self._batch_size\n",
        "    self._batch_id += 1\n",
        "\n",
        "    if end + self._batch_size > len(self._data):\n",
        "      self._size = end\n",
        "      end = len(self._data)\n",
        "      start = end - self._batch_size\n",
        "      self._num_epoch += 1\n",
        "      self._batch_id = 0\n",
        "      indices = list(range(len(self._data)))\n",
        "      random.seed(2021)\n",
        "      random.shuffle(indices)\n",
        "      self._data, self._labels, self._sentence_lengths, self._final_tokens = self._data[indices], self._labels[indices], self._sentence_lengths[indices], self._final_tokens[indices]\n",
        "\n",
        "    return self._data[start:end], self._labels[start:end], self._sentence_lengths[start:end], self._final_tokens[start:end]"
      ],
      "metadata": {
        "id": "U6kxgDnGrGFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizing\n",
        "loss_report = []\n",
        "accuracy_report = []\n",
        "\n",
        "\n",
        "def train_and_evaluate_RNN():\n",
        "  with open(vocab_path) as f:\n",
        "      vocab_size = len(f.read().splitlines())\n",
        "\n",
        "  tf.set_random_seed(2021)\n",
        "  rnn = RNN(\n",
        "      vocab_size=vocab_size,\n",
        "      embedding_size=300,\n",
        "      lstm_size=50,\n",
        "      batch_size=50\n",
        "  )\n",
        "  predicted_labels, loss = rnn.build_graph()\n",
        "  train_op = rnn.trainer(loss=loss, learning_rate=0.01)\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    train_data_reader = DataReader(\n",
        "        data_path='/Users/nguyennamhai/HUST/Training phase Machine Learning lab 2023/Phase 1/Session 4/Datasets/20news_train_encoded.txt',\n",
        "        batch_size=50,\n",
        "    )\n",
        "    test_data_reader = DataReader(\n",
        "        data_path='/Users/nguyennamhai/HUST/Training phase Machine Learning lab 2023/Phase 1/Session 4/Datasets/20news_test_encoded.txt',\n",
        "        batch_size=50,\n",
        "    )\n",
        "    step = 0\n",
        "    MAX_STEP = 10000\n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    while step < MAX_STEP:\n",
        "        next_train_batch = train_data_reader.next_batch()\n",
        "        train_data, train_labels, train_sentence_lengths, train_final_tokens = next_train_batch\n",
        "        plabels_eval, loss_eval, _ = sess.run(\n",
        "            [predicted_labels, loss, train_op],\n",
        "            feed_dict={\n",
        "                rnn._data: train_data,\n",
        "                rnn._labels: train_labels,\n",
        "                rnn._sentence_lengths: train_sentence_lengths,\n",
        "                rnn._final_tokens: train_final_tokens\n",
        "            }\n",
        "        )\n",
        "        step += 1\n",
        "        if step % 20 == 0:\n",
        "          loss_report.append(loss_eval)\n",
        "          print('loss: {}'.format(loss_eval))\n",
        "        if train_data_reader._batch_id == 0:\n",
        "          num_true_preds = 0\n",
        "          while True:\n",
        "            next_test_batch = test_data_reader.next_batch()\n",
        "            test_data, test_labels, test_sentence_lengths, test_final_tokens = next_test_batch\n",
        "            test_plabels_eval = sess.run(\n",
        "                predicted_labels,\n",
        "                feed_dict={\n",
        "                    rnn._data: test_data,\n",
        "                    rnn._labels: test_labels,\n",
        "                    rnn._sentence_lengths: test_sentence_lengths,\n",
        "                    rnn._final_tokens: test_final_tokens\n",
        "                }\n",
        "            )\n",
        "            matches = np.equal(test_plabels_eval, test_labels)\n",
        "            num_true_preds += np.sum(matches.astype(float))\n",
        "\n",
        "            if test_data_reader._batch_id == 0:\n",
        "              break\n",
        "\n",
        "          accuracy_report.append(num_true_preds * 100. / test_data_reader._size)        \n",
        "          print('Epoch: {}'.format(train_data_reader._num_epoch))\n",
        "          print('Accuracy on test data: {}'.format(num_true_preds * 100. / test_data_reader._size))"
      ],
      "metadata": {
        "id": "jkYT06S3rXo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate_RNN()"
      ],
      "metadata": {
        "id": "a4I4FvMert1q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}